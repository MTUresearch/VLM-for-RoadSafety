{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103718,
     "status": "ok",
     "timestamp": 1745460105217,
     "user": {
      "displayName": "Ali Mansouri",
      "userId": "12487842304850083253"
     },
     "user_tz": 240
    },
    "id": "dR3clgK-H9Y3",
    "outputId": "ab30489d-1db7-4f4a-bd8b-3fb4b01d5094"
   },
   "outputs": [],
   "source": [
    "!pip install open_clip_torch torch torchvision pillow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm9j6jmo6BlA"
   },
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47901,
     "status": "ok",
     "timestamp": 1743868007545,
     "user": {
      "displayName": "Ali Mansouri",
      "userId": "12487842304850083253"
     },
     "user_tz": 240
    },
    "id": "CdnkYMidfzJX",
    "outputId": "3799797e-2984-4a60-ca31-0d23e753a019"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "yes_folder = '/content/drive/MyDrive/Group-Publications/VLM - Safety/Guardrail/Yes'  # Update this path\n",
    "no_folder = '/content/drive/MyDrive/Group-Publications/VLM - Safety/Guardrail/No'    # Update this path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208,
     "referenced_widgets": [
      "f9dcefdbbca7426e87a913e902031f52",
      "f9312d21c56c47dabd47138a87b78a89",
      "2996917b26b54f4d883ffe1f3b86d3ee",
      "b5341cd292e14b62b784ac655618752a",
      "89d27dbec2a748eb9ad75490f0a7276c",
      "dc096a18ebd74504bef62e66e5b4aea4",
      "ce05d1b4d1574bd79c6548036f4ecc27",
      "78896ca8834b4afc98ad550421fd74b1",
      "88ae57dd52fa47fea1b2c0a2ee0475eb",
      "a5a8c0ee828145439445bdb5866c3e9e",
      "d82e75bfd9cf44e2a999a0418ad0c381"
     ]
    },
    "executionInfo": {
     "elapsed": 101938,
     "status": "ok",
     "timestamp": 1743868109481,
     "user": {
      "displayName": "Ali Mansouri",
      "userId": "12487842304850083253"
     },
     "user_tz": 240
    },
    "id": "ltoybTnshtGV",
    "outputId": "7b4a1d72-37ee-4136-c328-b574029112fa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "from google.colab import files\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the OpenCLIP ViT-L/14 model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess, _ = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')  # Fix here\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')\n",
    "\n",
    "# Define the text prompts\n",
    "text_labels = [\"A highway with a sturdy metal guardrail for vehicle safety.\", \"An open road without a guardrail, exposing vehicles to roadside hazards.\"]\n",
    "text_inputs = tokenizer(text_labels).to(device)\n",
    "\n",
    "def classify_image(image_path):\n",
    "    \"\"\"\n",
    "    Classifies whether an image contains a guardrail or not using OpenCLIP ViT-L/14.\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Run model inference\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "        # Normalize features\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute similarity\n",
    "        similarity = (image_features @ text_features.T).squeeze(0)\n",
    "        probs = similarity.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    # Print results\n",
    "    model_label = \"yes\" if probs[0] > probs[1] else \"no\"\n",
    "    return model_label, probs[0], probs[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0nepw4qgbQ-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "results = []\n",
    "\n",
    "def process_folder(folder_path, true_label):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            image_id = os.path.splitext(filename)[0]\n",
    "            model_label, prob_yes, prob_no = classify_image(image_path)\n",
    "            results.append({\n",
    "                \"ID\": image_id,\n",
    "                \"label\": true_label,\n",
    "                \"model label\": model_label,\n",
    "                \"prob. yes\": prob_yes,\n",
    "                \"prob. no\": prob_no\n",
    "            })\n",
    "\n",
    "# Process both folders\n",
    "process_folder(yes_folder, \"yes\")\n",
    "process_folder(no_folder, \"no\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1743871286936,
     "user": {
      "displayName": "Ali Mansouri",
      "userId": "12487842304850083253"
     },
     "user_tz": 240
    },
    "id": "yaTPfsNWgeAF",
    "outputId": "26bbe189-4b1d-4138-8a12-7c2c36b116b7"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "for row in results:\n",
    "    row[\"correct\"] = 1 if row[\"label\"] == row[\"model label\"] else 0\n",
    "\n",
    "# Save results to Excel\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = df[\"correct\"].sum() / len(df)\n",
    "\n",
    "# Create an empty row with only accuracy in the 'correct' column\n",
    "summary_row = {col: \"\" for col in df.columns}\n",
    "summary_row[\"label\"] = f\"Accuracy: {accuracy:.4f}\"\n",
    "\n",
    "# Append the row to the DataFrame\n",
    "df = pd.concat([df, pd.DataFrame([summary_row])], ignore_index=True)\n",
    "\n",
    "df[\"ID\"] = pd.to_numeric(df[\"ID\"], errors='coerce').astype('Int64')\n",
    "df[\"correct\"] = pd.to_numeric(df[\"correct\"], errors='coerce').astype('Int64')\n",
    "df[\"prob. yes\"] = pd.to_numeric(df[\"prob. yes\"], errors='coerce').astype(float)\n",
    "df[\"prob. no\"] = pd.to_numeric(df[\"prob. no\"], errors='coerce').astype(float)\n",
    "\n",
    "\n",
    "# Create timestamp\n",
    "est_time = datetime.now(ZoneInfo(\"America/New_York\"))\n",
    "timestamp = est_time.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "filename = f\"/content/drive/MyDrive/Group-Publications/VLM - Safety/results_{timestamp}.xlsx\"\n",
    "\n",
    "df.to_excel(filename, index=False)\n",
    "\n",
    "print(f\"Classification complete and results saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3L_3l6eDjMfZ"
   },
   "outputs": [],
   "source": [
    "# Upload multiple images from local PC\n",
    "uploaded = files.upload()  # Opens file selector, allowing multiple selections\n",
    "\n",
    "# Loop through all uploaded images and classify each one\n",
    "for image_path in uploaded.keys():\n",
    "    print(f\"\\nProcessing: {image_path}\")\n",
    "    classify_image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSRzQ3FX6LlC"
   },
   "source": [
    "# Co-Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8pTntUq6RfF"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "yes_folder = r'C:/Users/alimanso/anaconda_projects/VLM/Few_Shot_Yes/'\n",
    "no_folder = r'C:/Users/alimanso/anaconda_projects/VLM/Few_Shot_No/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCTNJDiDJNxA",
    "outputId": "1598d3dd-8558-4e83-a4b4-42388642c2c3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXaIRhU6JNxB"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8RJhgQvJNxB"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208,
     "referenced_widgets": [
      "178f9b64775940ffb4277b089693132f",
      "c51b5d8de13f4722bbc466ac284472b0",
      "9f713e431c5c4532ab9639e2c5c469bd",
      "2ead6645b89641e1bb59c74cc8ea7793",
      "859fdcc42ec34428851ce92b46aee758",
      "0f4990640e4046cab80989203637e92a",
      "f7e0b97b58964c8e825782891af8f0cd",
      "1e13b88e4a8343ab936cdfc647f431e1",
      "c1544c548d3b4f7ba6a78c4fd2667b9f",
      "99eda6d7faac4b16a9eca00041f011c9",
      "9fe7c0e551114e77933a02d03bf26788",
      "772a9a727d594e73bc62d1c12bc12505"
     ]
    },
    "executionInfo": {
     "elapsed": 46598,
     "status": "ok",
     "timestamp": 1745460278552,
     "user": {
      "displayName": "Ali Mansouri",
      "userId": "12487842304850083253"
     },
     "user_tz": 240
    },
    "id": "SS-rwY4U6nid",
    "outputId": "db204d24-ad33-4f1b-fd8f-1fcf9757e8fe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import random\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, preprocess, _ = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n",
    "model = model.to(device)\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrwbQaUxRMfa"
   },
   "source": [
    "## Define Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "error",
     "timestamp": 1746635329743,
     "user": {
      "displayName": "Ali Mansouri",
      "userId": "12487842304850083253"
     },
     "user_tz": 240
    },
    "id": "ffEmsT9dyHPE",
    "outputId": "464a5ccd-70d4-4be6-86f0-8af11de5a89e"
   },
   "outputs": [],
   "source": [
    "classnames = [\"guardrail\", \"no guardrail\"]\n",
    "print(\"token_embedding shape:\", model.token_embedding.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWxHvCv5POTF"
   },
   "source": [
    "## CoOp Prompt Learner --> Unified Context (Option 1)\n",
    "(Similar learned vectors for both classes) (End-Position Class Token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbU6dnIA6prv"
   },
   "outputs": [],
   "source": [
    "# ----- CoOp Prompt Learner -----Unified Context version\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, classnames, ctx_len=4, tokenizer=None, clip_model=None):\n",
    "        super().__init__()\n",
    "        self.ctx_len = ctx_len\n",
    "        self.classnames = classnames\n",
    "        self.tokenizer = tokenizer\n",
    "        self.clip_model = clip_model\n",
    "        self.device = next(clip_model.parameters()).device\n",
    "\n",
    "        # Get token embedding dimension\n",
    "        self.embed_dim = clip_model.token_embedding.weight.shape[1]\n",
    "\n",
    "        # Init learnable context: (ctx_len, embed_dim)\n",
    "        self.ctx = nn.Parameter(torch.randn(ctx_len, self.embed_dim) * 0.02)\n",
    "\n",
    "        # Tokenize classnames (IDs)\n",
    "        tokenized = [tokenizer(f\"{name}\") for name in classnames]  # no template\n",
    "        self.tokenized = torch.cat(tokenized).reshape(len(classnames), -1).to(self.device)  # shape: [num_classes, seq_len]\n",
    "\n",
    "        # Setup special tokens (e.g., [SOS] and [EOS])\n",
    "        self.sos_id = tokenizer(\"X\")[0][0].item()  # any token will do\n",
    "        self.eos_id = 49407 if clip_model.token_embedding.num_embeddings >= 49408 else 0  # depends on tokenizer vocab size\n",
    "        self.total_prompt_len = ctx_len + self.tokenized.shape[1] + 2  # [SOS] + ctx + class + [EOS]\n",
    "\n",
    "    def forward(self):\n",
    "        B = len(self.classnames)\n",
    "        ctx = self.ctx.unsqueeze(0).expand(B, -1, -1)  # shape: [B, ctx_len, D]\n",
    "\n",
    "        # Token embeddings\n",
    "        class_embeds = self.clip_model.token_embedding(self.tokenized)  # [B, class_len, D]\n",
    "        sos = self.clip_model.token_embedding.weight[self.sos_id].unsqueeze(0).unsqueeze(0).expand(B, 1, -1)\n",
    "        eos = self.clip_model.token_embedding.weight[self.eos_id].unsqueeze(0).unsqueeze(0).expand(B, 1, -1)\n",
    "\n",
    "        # Final prompt: [SOS] + [ctx] + [class tokens] + [EOS]\n",
    "        prompt_embeds = torch.cat([sos, ctx, class_embeds, eos], dim=1)  # [B, total_len, D]\n",
    "        return prompt_embeds\n",
    "\n",
    "\n",
    "prompt_learner = PromptLearner(\n",
    "    classnames=classnames,\n",
    "    ctx_len=8,\n",
    "    tokenizer=tokenizer,\n",
    "    clip_model=model\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yV_ghYKePdGc"
   },
   "source": [
    "## CoOp Prompt Learner --> Class-Specific Context (Option 2)\n",
    "(Distinct learned vectors for each class) (End-Position Class Token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUtbSS2T0pTQ"
   },
   "outputs": [],
   "source": [
    "# ----- CoOp Prompt Learner -----Class-Specific Context\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, classnames, ctx_len=4, tokenizer=None, clip_model=None):\n",
    "        super().__init__()\n",
    "        self.ctx_len = ctx_len\n",
    "        self.classnames = classnames\n",
    "        self.tokenizer = tokenizer\n",
    "        self.clip_model = clip_model\n",
    "        self.device = next(clip_model.parameters()).device\n",
    "\n",
    "        self.embed_dim = clip_model.token_embedding.weight.shape[1]\n",
    "\n",
    "        # Class-specific learnable context: [num_classes, ctx_len, embed_dim]\n",
    "        self.ctx = nn.Parameter(torch.randn(len(classnames), ctx_len, self.embed_dim) * 0.02)\n",
    "\n",
    "        # Tokenize classnames (IDs)\n",
    "        tokenized = [tokenizer(f\"{name}\") for name in classnames]\n",
    "        self.tokenized = torch.cat(tokenized).reshape(len(classnames), -1).to(self.device)\n",
    "\n",
    "        self.sos_id = tokenizer(\"X\")[0][0].item()\n",
    "        self.eos_id = 49407 if clip_model.token_embedding.num_embeddings >= 49408 else 0\n",
    "        self.total_prompt_len = ctx_len + self.tokenized.shape[1] + 2\n",
    "\n",
    "    def forward(self):\n",
    "        B = len(self.classnames)\n",
    "\n",
    "        # Token embeddings for each class\n",
    "        class_embeds = self.clip_model.token_embedding(self.tokenized)  # [B, class_len, D]\n",
    "        sos = self.clip_model.token_embedding.weight[self.sos_id].unsqueeze(0).unsqueeze(0).expand(B, 1, -1)\n",
    "        eos = self.clip_model.token_embedding.weight[self.eos_id].unsqueeze(0).unsqueeze(0).expand(B, 1, -1)\n",
    "\n",
    "        # Use class-specific context\n",
    "        ctx = self.ctx  # shape: [B, ctx_len, D]\n",
    "\n",
    "        prompt_embeds = torch.cat([sos, ctx, class_embeds, eos], dim=1)  # [B, total_len, D]\n",
    "        return prompt_embeds\n",
    "\n",
    "\n",
    "prompt_learner = PromptLearner(\n",
    "    classnames=classnames,\n",
    "    ctx_len=8,\n",
    "    tokenizer=tokenizer,\n",
    "    clip_model=model\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb_RA4lGQNuc"
   },
   "source": [
    "## CoOp Prompt Learner --> Class-Specific Context (Option 3)\n",
    "(Distinct learned vectors for each class) (Mid-Position Class Token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DdRvJOIJNxC"
   },
   "outputs": [],
   "source": [
    "# ----- CoOp Prompt Learner -----Class-Specific Context class in middle\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, classnames, ctx_len=4, tokenizer=None, clip_model=None):\n",
    "        super().__init__()\n",
    "        self.ctx_len = ctx_len\n",
    "        self.classnames = classnames\n",
    "        self.tokenizer = tokenizer\n",
    "        self.clip_model = clip_model\n",
    "        self.device = next(clip_model.parameters()).device\n",
    "\n",
    "        self.embed_dim = clip_model.token_embedding.weight.shape[1]\n",
    "\n",
    "        # Class-specific learnable context: [num_classes, ctx_len, embed_dim]\n",
    "        self.ctx = nn.Parameter(torch.randn(len(classnames), ctx_len, self.embed_dim) * 0.02)\n",
    "\n",
    "        # Tokenize classnames (IDs)\n",
    "        tokenized = [tokenizer(f\"{name}\") for name in classnames]\n",
    "        self.tokenized = torch.cat(tokenized).reshape(len(classnames), -1).to(self.device)\n",
    "\n",
    "        self.sos_id = tokenizer(\"X\")[0][0].item()\n",
    "        self.eos_id = 49407 if clip_model.token_embedding.num_embeddings >= 49408 else 0\n",
    "        self.total_prompt_len = ctx_len + self.tokenized.shape[1] + 2\n",
    "\n",
    "    def forward(self):\n",
    "        B = len(self.classnames)\n",
    "        ctx_half = self.ctx_len // 2\n",
    "\n",
    "        # Split each class-specific context into prefix/suffix\n",
    "        ctx_prefix = self.ctx[:, :ctx_half, :]  # [B, ctx_half, D]\n",
    "        ctx_suffix = self.ctx[:, ctx_half:, :]  # [B, ctx_half, D]\n",
    "\n",
    "        # Class token embeddings: [B, class_len, D]\n",
    "        class_embeds = self.clip_model.token_embedding(self.tokenized)\n",
    "\n",
    "        # Special tokens\n",
    "        sos = self.clip_model.token_embedding.weight[self.sos_id].unsqueeze(0).unsqueeze(0).expand(B, 1, -1)\n",
    "        eos = self.clip_model.token_embedding.weight[self.eos_id].unsqueeze(0).unsqueeze(0).expand(B, 1, -1)\n",
    "\n",
    "        # Combine: [SOS] + ctx_prefix + [CLASS] + ctx_suffix + [EOS]\n",
    "        prompt_embeds = torch.cat([sos, ctx_prefix, class_embeds, ctx_suffix, eos], dim=1)\n",
    "        return prompt_embeds\n",
    "\n",
    "\n",
    "\n",
    "prompt_learner = PromptLearner(\n",
    "    classnames=classnames,\n",
    "    ctx_len=8,\n",
    "    tokenizer=tokenizer,\n",
    "    clip_model=model\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeLjgxc9QbOD"
   },
   "source": [
    "## Define Few Shot Limit and Split Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-PZm_zM6us7"
   },
   "outputs": [],
   "source": [
    "# === Dataset with Manual Split ===\n",
    "def split_guardrail_dataset(yes_dir, no_dir, shot_limit, seed=42):\n",
    "    random.seed(seed)\n",
    "    yes_imgs = [os.path.join(yes_dir, f) for f in os.listdir(yes_dir) if f.endswith(('.jpg', '.png'))]\n",
    "    no_imgs = [os.path.join(no_dir, f) for f in os.listdir(no_dir) if f.endswith(('.jpg', '.png'))]\n",
    "    yes_train = random.sample(yes_imgs, shot_limit)\n",
    "    no_train = random.sample(no_imgs, shot_limit)\n",
    "    yes_test = list(set(yes_imgs) - set(yes_train))\n",
    "    no_test = list(set(no_imgs) - set(no_train))\n",
    "    train_samples = [(p, 0) for p in yes_train] + [(p, 1) for p in no_train]\n",
    "    test_samples = [(p, 0) for p in yes_test] + [(p, 1) for p in no_test]\n",
    "    random.shuffle(train_samples)\n",
    "    random.shuffle(test_samples)\n",
    "    return train_samples, test_samples\n",
    "\n",
    "class GuardrailDataset(Dataset):\n",
    "    def __init__(self, samples, transform):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        return self.transform(image), label\n",
    "\n",
    "\n",
    "shot_limit = 8  # Try 4, 8, or 16\n",
    "train_samples, test_samples = split_guardrail_dataset(yes_folder, no_folder, shot_limit)\n",
    "\n",
    "train_dataset = GuardrailDataset(train_samples, transform=preprocess)\n",
    "test_dataset = GuardrailDataset(test_samples, transform=preprocess)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqQM6lPrQxgb"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "executionInfo": {
     "elapsed": 7265,
     "status": "error",
     "timestamp": 1745460366327,
     "user": {
      "displayName": "Ali Mansouri",
      "userId": "12487842304850083253"
     },
     "user_tz": 240
    },
    "id": "evl8u_T86yeN",
    "outputId": "be0af237-a4fd-4298-f62e-8dc66e7a5506"
   },
   "outputs": [],
   "source": [
    "# === Training ===\n",
    "def encode_custom_prompt(prompts):\n",
    "    x = model.transformer(prompts)\n",
    "    x = x[torch.arange(x.shape[0]), -1]\n",
    "    return x @ model.text_projection\n",
    "\n",
    "def train(train_loader, prompt_learner, epochs=100):\n",
    "    optimizer = torch.optim.AdamW([prompt_learner.ctx], lr=5e-4, weight_decay=0.01)\n",
    "    model.eval()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                image_features = model.encode_image(images)\n",
    "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            prompts = prompt_learner()\n",
    "            text_features = encode_custom_prompt(prompts)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            logits = image_features @ text_features.T\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "train(train_loader, prompt_learner, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkrj97lhQ3va"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1745450590449,
     "user": {
      "displayName": "Ali Mansouri",
      "userId": "12487842304850083253"
     },
     "user_tz": 240
    },
    "id": "wqLj0X6860UA",
    "outputId": "6e58a452-540e-45e7-c46e-c1835c70eb57"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# === Evaluation ===\n",
    "def classify_tensor_image(image_tensor):\n",
    "    image_tensor = image_tensor.to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_tensor)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        prompts = prompt_learner()\n",
    "        x = model.transformer(prompts)\n",
    "        x = x[torch.arange(x.shape[0]), -1]\n",
    "        text_features = x @ model.text_projection\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (image_features @ text_features.T).squeeze(0)\n",
    "        probs = similarity.softmax(dim=-1).cpu().numpy()\n",
    "    model_label = \"yes\" if probs[0] > probs[1] else \"no\"\n",
    "    return model_label, probs[0], probs[1]\n",
    "\n",
    "def evaluate(test_dataset):\n",
    "    results = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    for (image_tensor, label), (img_path, _) in zip(loader, test_dataset.samples):\n",
    "        model_label, prob_yes, prob_no = classify_tensor_image(image_tensor.squeeze(0))\n",
    "        #model_label, prob_yes, prob_no = classify_tensor_image(image.squeeze(0))\n",
    "        gt = \"yes\" if label.item() == 0 else \"no\"\n",
    "        pred_label = model_label\n",
    "        image_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "        results.append({\n",
    "            \"ID\": image_id,\n",
    "            \"label\": gt,\n",
    "            \"model label\": pred_label,\n",
    "            \"prob. yes\": prob_yes,\n",
    "            \"prob. no\": prob_no,\n",
    "            \"correct\": int(model_label == gt)\n",
    "        })\n",
    "\n",
    "        y_true.append(0 if gt == \"yes\" else 1)\n",
    "        y_pred.append(0 if pred_label == \"yes\" else 1)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    accuracy = df[\"correct\"].sum() / len(df)\n",
    "    summary_row = {col: \"\" for col in df.columns}\n",
    "    summary_row[\"label\"] = f\"Accuracy: {accuracy:.4f}\"\n",
    "    df = pd.concat([df, pd.DataFrame([summary_row])], ignore_index=True)\n",
    "\n",
    "    df[\"ID\"] = pd.to_numeric(df[\"ID\"], errors='coerce').astype('Int64')\n",
    "    df[\"correct\"] = pd.to_numeric(df[\"correct\"], errors='coerce').astype('Int64')\n",
    "    df[\"prob. yes\"] = pd.to_numeric(df[\"prob. yes\"], errors='coerce').astype(float)\n",
    "    df[\"prob. no\"] = pd.to_numeric(df[\"prob. no\"], errors='coerce').astype(float)\n",
    "\n",
    "    timestamp = datetime.now(ZoneInfo(\"America/New_York\")).strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    filename = 'C:/Users/alimanso/anaconda_projects/VLM/results_' + timestamp + '.xlsx'\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "    print(f\"\\nðŸ“Š Final Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"âœ… Results saved to: {filename}\")\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"yes\", \"no\"])\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f4990640e4046cab80989203637e92a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "178f9b64775940ffb4277b089693132f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c51b5d8de13f4722bbc466ac284472b0",
       "IPY_MODEL_9f713e431c5c4532ab9639e2c5c469bd",
       "IPY_MODEL_2ead6645b89641e1bb59c74cc8ea7793"
      ],
      "layout": "IPY_MODEL_859fdcc42ec34428851ce92b46aee758"
     }
    },
    "1e13b88e4a8343ab936cdfc647f431e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2996917b26b54f4d883ffe1f3b86d3ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78896ca8834b4afc98ad550421fd74b1",
      "max": 1710517724,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88ae57dd52fa47fea1b2c0a2ee0475eb",
      "value": 1710517724
     }
    },
    "2ead6645b89641e1bb59c74cc8ea7793": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99eda6d7faac4b16a9eca00041f011c9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9fe7c0e551114e77933a02d03bf26788",
      "value": "â€‡1.71G/1.71Gâ€‡[00:14&lt;00:00,â€‡117MB/s]"
     }
    },
    "78896ca8834b4afc98ad550421fd74b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "859fdcc42ec34428851ce92b46aee758": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88ae57dd52fa47fea1b2c0a2ee0475eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "89d27dbec2a748eb9ad75490f0a7276c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99eda6d7faac4b16a9eca00041f011c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f713e431c5c4532ab9639e2c5c469bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e13b88e4a8343ab936cdfc647f431e1",
      "max": 1710517724,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c1544c548d3b4f7ba6a78c4fd2667b9f",
      "value": 1710517724
     }
    },
    "9fe7c0e551114e77933a02d03bf26788": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a5a8c0ee828145439445bdb5866c3e9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5341cd292e14b62b784ac655618752a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5a8c0ee828145439445bdb5866c3e9e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d82e75bfd9cf44e2a999a0418ad0c381",
      "value": "â€‡1.71G/1.71Gâ€‡[00:16&lt;00:00,â€‡174MB/s]"
     }
    },
    "c1544c548d3b4f7ba6a78c4fd2667b9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c51b5d8de13f4722bbc466ac284472b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f4990640e4046cab80989203637e92a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f7e0b97b58964c8e825782891af8f0cd",
      "value": "open_clip_model.safetensors:â€‡100%"
     }
    },
    "ce05d1b4d1574bd79c6548036f4ecc27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d82e75bfd9cf44e2a999a0418ad0c381": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc096a18ebd74504bef62e66e5b4aea4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7e0b97b58964c8e825782891af8f0cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9312d21c56c47dabd47138a87b78a89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc096a18ebd74504bef62e66e5b4aea4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ce05d1b4d1574bd79c6548036f4ecc27",
      "value": "open_clip_model.safetensors:â€‡100%"
     }
    },
    "f9dcefdbbca7426e87a913e902031f52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f9312d21c56c47dabd47138a87b78a89",
       "IPY_MODEL_2996917b26b54f4d883ffe1f3b86d3ee",
       "IPY_MODEL_b5341cd292e14b62b784ac655618752a"
      ],
      "layout": "IPY_MODEL_89d27dbec2a748eb9ad75490f0a7276c"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
