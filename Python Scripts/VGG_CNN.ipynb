{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak-dae9YY4Fu"
   },
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XS4GP4f0Fom9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import base64\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "from google.colab import userdata\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Install dependencies\n",
    "!pip install xgboost seaborn --quiet\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.layers import Flatten, Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0KOB2mWY92s"
   },
   "source": [
    "# Connecting to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNPFhRnJ-0Fa",
    "outputId": "36b16210-e492-4c63-877d-77dd0cab2430"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fl-i9ntkFqLi"
   },
   "outputs": [],
   "source": [
    "# Unzip dataset\n",
    "zip_path = \"/content/drive/MyDrive/Data2.zip\"\n",
    "unzip_dir = \"/content/data2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dM7_upm8GMN2"
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(unzip_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjZJh0EoIhYL",
    "outputId": "90c2cafa-cf1a-4b01-a576-86184e365ad3"
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPYS-rGGcEjL"
   },
   "source": [
    "# Training No.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTIpDKCIGMus",
    "outputId": "3c61bb0f-f884-4eb8-94e6-b14e7fbc0ca6"
   },
   "outputs": [],
   "source": [
    "# --- 1. Load and preprocess images\n",
    "def load_images_from_folder(folder, label, image_size=(128,128)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            img = load_img(img_path, target_size=image_size)\n",
    "            img_array = img_to_array(img)\n",
    "            images.append(img_array)\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "# Example usage:\n",
    "# Replace these paths with your actual dataset folders\n",
    "class1_images, class1_labels = load_images_from_folder('/content/data2/Data/Data/Guardrail/No', label=0)\n",
    "class2_images, class2_labels = load_images_from_folder('/content/data2/Data/Data/Guardrail/Yes', label=1)\n",
    "\n",
    "X = np.array(class1_images + class2_images)\n",
    "y = np.array(class1_labels + class2_labels)\n",
    "\n",
    "# Normalize images as VGG16 expects inputs in range 0-255, we preprocess later\n",
    "# Optionally, you can scale here or inside feature extractor preprocessing\n",
    "\n",
    "# --- 2. Prepare VGG16 feature extractor ---\n",
    "input_tensor = Input(shape=(128,128,3))\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "x = Flatten()(base_model.output)\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# VGG16 preprocessing function\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Preprocess input images\n",
    "X_preprocessed = preprocess_input(X)\n",
    "\n",
    "# --- 3. Extract features ---\n",
    "features = feature_extractor.predict(X_preprocessed, batch_size=32, verbose=1)\n",
    "\n",
    "# --- 4. Split data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# --- 5. Train XGBoost classifier ---\n",
    "xgb_clf = xgb.XGBClassifier(max_depth=4, learning_rate=0.1, subsample=0.85,\n",
    "                            colsample_bytree=0.9, gamma=0.1, n_estimators=200,\n",
    "                            use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# --- 6. Evaluate ---\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1Yf5rM5KJIlK",
    "outputId": "cd2274cf-52d2-4277-a3c8-867288385da9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. Load and preprocess images ---\n",
    "def load_images_from_folder(folder, label, image_size=(128,128)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            img = load_img(img_path, target_size=image_size)\n",
    "            img_array = img_to_array(img)\n",
    "            images.append(img_array)\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "# Replace these with your dataset paths\n",
    "class0_images, class0_labels = load_images_from_folder('/content/data2/Data/Data/Guardrail/No', label=0)\n",
    "class1_images, class1_labels = load_images_from_folder('/content/data2/Data/Data/Guardrail/Yes', label=1)\n",
    "\n",
    "X = np.array(class0_images + class1_images)\n",
    "y = np.array(class0_labels + class1_labels)\n",
    "\n",
    "# --- 2. Prepare VGG16 feature extractor ---\n",
    "input_tensor = Input(shape=(128,128,3))\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "x = Flatten()(base_model.output)\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Preprocess images for VGG16\n",
    "X_preprocessed = preprocess_input(X)\n",
    "\n",
    "# --- 3. Extract features ---\n",
    "features = feature_extractor.predict(X_preprocessed, batch_size=32, verbose=1)\n",
    "\n",
    "# --- 4. Split data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.2,\n",
    "                                                    random_state=42, stratify=y)\n",
    "\n",
    "# --- 5. Train XGBoost classifier with evaluation logging ---\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "xgb_clf = xgb.XGBClassifier(max_depth=4, learning_rate=0.1, subsample=0.85,\n",
    "                            colsample_bytree=0.9, gamma=0.1, n_estimators=200,\n",
    "                            use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_clf.fit(X_train, y_train, eval_set=eval_set, verbose=True)\n",
    "\n",
    "# --- 6. Plot training and test logloss ---\n",
    "results = xgb_clf.evals_result()\n",
    "epochs = len(results['validation_0']['logloss'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "plt.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('XGBoost Log Loss During Training')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- 7. Evaluate accuracy and print classification report ---\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# --- 8. Plot confusion matrix ---\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "OAkxjQPCMzJP",
    "outputId": "dce2a069-0db5-4d60-aafa-2a3577f0e199"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the first tree (tree number 0)\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(xgb_clf, num_trees=0, rankdir='LR')  # rankdir='LR' for left-to-right layout\n",
    "plt.title('XGBoost Decision Tree (Tree 0)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "Bguzl--uOBjj",
    "outputId": "9aa7c04d-83e5-46af-f18f-1287b070e944"
   },
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plot_importance(xgb_clf, max_num_features=20, importance_type='weight')  # or 'gain', 'cover'\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4SvTXPf6OHTp",
    "outputId": "6127d450-7685-4a01-f326-c7cbe9def8e5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(feature_extractor, to_file='vgg16_feature_extractor.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b70lfsc2du78"
   },
   "source": [
    "# Training No.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5Kp5b74Hixar",
    "outputId": "b4fcac0d-f642-4914-c8dd-59625d8b734d"
   },
   "outputs": [],
   "source": [
    "# --- 1. Load and preprocess images ---\n",
    "def load_images_from_folder(folder, label, image_size=(128,128)):\n",
    "    images, labels = [], []\n",
    "    for fn in os.listdir(folder):\n",
    "        if fn.lower().endswith(('.jpg','.png')):\n",
    "            img = load_img(os.path.join(folder, fn), target_size=image_size)\n",
    "            images.append(img_to_array(img))\n",
    "            labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Paths\n",
    "o_dir = '/content/data2/Data/Data/Guardrail/No'\n",
    "yes_dir = '/content/data2/Data/Data/Guardrail/Yes'\n",
    "\n",
    "X0, y0 = load_images_from_folder(o_dir, 0)\n",
    "X1, y1 = load_images_from_folder(yes_dir, 1)\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.hstack([y0, y1])\n",
    "\n",
    "# --- 2. Train/Val/Test split (60/20/20) ---\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Preprocess\n",
    "X_train_p = preprocess_input(X_train)\n",
    "X_val_p   = preprocess_input(X_val)\n",
    "X_test_p  = preprocess_input(X_test)\n",
    "\n",
    "# --- 3. Fine-tune VGG16 as feature learner ---\n",
    "input_tensor = Input(shape=(128,128,3))\n",
    "base = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "# Freeze first N layers, unfreeze rest\n",
    "for layer in base.layers[:10]:\n",
    "    layer.trainable = False\n",
    "for layer in base.layers[10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Add classification head for fine-tuning\n",
    "x = Flatten()(base.output)\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "ft_model = Model(base.input, out)\n",
    "\n",
    "ft_model.compile(\n",
    "    optimizer=Adam(1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "ckpt = ModelCheckpoint('best_ft.h5', save_best_only=True)\n",
    "\n",
    "# Train (capture history for plots)\n",
    "history = ft_model.fit(\n",
    "    X_train_p, y_train,\n",
    "    validation_data=(X_val_p, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[es, ckpt],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- 4. Reload best weights and repurpose base for feature extraction ---\n",
    "ft_model.load_weights('best_ft.h5')\n",
    "for layer in base.layers:\n",
    "    layer.trainable = False\n",
    "feat_extractor = Model(base.input, Flatten()(base.output))\n",
    "\n",
    "ModelCheckpoint('best_ft.keras', save_best_only=True)\n",
    "\n",
    "# --- 5. Extract features for XGBoost ---\n",
    "feat_train = feat_extractor.predict(X_train_p, batch_size=32)\n",
    "feat_val   = feat_extractor.predict(X_val_p, batch_size=32)\n",
    "feat_test  = feat_extractor.predict(X_test_p, batch_size=32)\n",
    "\n",
    "# --- 6. Hyperparameter tuning with RandomizedSearchCV (faster than full grid) ---\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'max_depth': [3,4,5],\n",
    "    'learning_rate': [0.01,0.1],\n",
    "    'n_estimators': [50,100,200],\n",
    "    'gamma': [0,0.1,1]\n",
    "}\n",
    "xgb_base = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "rand_search = RandomizedSearchCV(\n",
    "    xgb_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "# rand_search.fit(Xgb_X, Xgb_y)\n",
    "rand_search.fit(feat_train, y_train)\n",
    "best_xgb = rand_search.best_estimator_\n",
    "print(\"Best XGB params:\", rand_search.best_params_)\n",
    "\n",
    "# --- 7. Final evaluation on test set ---\n",
    "# Use the best model from randomized search\n",
    "feat_test  # ensure feat_test is computed above\n",
    "\n",
    "y_pred = best_xgb.predict(feat_test)\n",
    "y_proba = best_xgb.predict_proba(feat_test)[:,1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --- 8. Plots ---. Plots ---\n",
    "\n",
    "# 8.1 Loss & Accuracy for CNN fine-tuning\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('CNN Fine-tuning Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('CNN Fine-tuning Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8.2 ROC Curve for XGBoost\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_disp = RocCurveDisplay(fpr=fpr, tpr=tpr)\n",
    "roc_disp.plot()\n",
    "plt.title('XGBoost ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# 8.3 Confusion Matrix for XGBoost\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "cm_disp.plot(cmap='Blues')\n",
    "plt.title('XGBoost Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQNW9KALgY6a"
   },
   "source": [
    "# Traning No.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "K5T_nYxCjkpI",
    "outputId": "134fa53b-cd4b-42ff-9361-26040546ae83"
   },
   "outputs": [],
   "source": [
    "# --- 1. Load and preprocess images ---\n",
    "def load_images_from_folder(folder, label, image_size=(128,128)):\n",
    "    images, labels = [], []\n",
    "    for fn in os.listdir(folder):\n",
    "        if fn.lower().endswith(('.jpg','.png')):\n",
    "            img = load_img(os.path.join(folder, fn), target_size=image_size)\n",
    "            images.append(img_to_array(img))\n",
    "            labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Paths\n",
    "o_dir = '/content/data2/Data/Data/Guardrail/No'\n",
    "yes_dir = '/content/data2/Data/Data/Guardrail/Yes'\n",
    "\n",
    "X0, y0 = load_images_from_folder(o_dir, 0)\n",
    "X1, y1 = load_images_from_folder(yes_dir, 1)\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.hstack([y0, y1])\n",
    "\n",
    "# --- 2. Train/Val/Test split (60/20/20) ---\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Preprocess\n",
    "X_train_p = preprocess_input(X_train)\n",
    "X_val_p   = preprocess_input(X_val)\n",
    "X_test_p  = preprocess_input(X_test)\n",
    "\n",
    "# --- 3. Fine-tune VGG16 as feature learner ---\n",
    "input_tensor = Input(shape=(128,128,3))\n",
    "base = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "# Freeze first N layers, unfreeze rest\n",
    "for layer in base.layers[:10]:\n",
    "    layer.trainable = False\n",
    "for layer in base.layers[10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Add classification head for fine-tuning\n",
    "x = Flatten()(base.output)\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "ft_model = Model(base.input, out)\n",
    "\n",
    "ft_model.compile(\n",
    "    optimizer=Adam(1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "ckpt = ModelCheckpoint('best_ft.h5', save_best_only=True)\n",
    "\n",
    "# Train (capture history for plots)\n",
    "history = ft_model.fit(\n",
    "    X_train_p, y_train,\n",
    "    validation_data=(X_val_p, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[es, ckpt],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- 4. Reload best weights and repurpose base for feature extraction ---\n",
    "ft_model.load_weights('best_ft.h5')\n",
    "for layer in base.layers:\n",
    "    layer.trainable = False\n",
    "feat_extractor = Model(base.input, Flatten()(base.output))\n",
    "\n",
    "ModelCheckpoint('best_ft.keras', save_best_only=True)\n",
    "\n",
    "# --- 5. Extract features for XGBoost ---\n",
    "feat_train = feat_extractor.predict(X_train_p, batch_size=32)\n",
    "feat_val   = feat_extractor.predict(X_val_p, batch_size=32)\n",
    "feat_test  = feat_extractor.predict(X_test_p, batch_size=32)\n",
    "\n",
    "# --- 6. Hyperparameter tuning with RandomizedSearchCV (faster than full grid) ---\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'max_depth': [3,4,5],\n",
    "    'learning_rate': [0.01,0.1],\n",
    "    'n_estimators': [50,100,200],\n",
    "    'gamma': [0,0.1,1]\n",
    "}\n",
    "xgb_base = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "rand_search = RandomizedSearchCV(\n",
    "    xgb_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "# rand_search.fit(Xgb_X, Xgb_y)\n",
    "rand_search.fit(feat_train, y_train)\n",
    "best_xgb = rand_search.best_estimator_\n",
    "print(\"Best XGB params:\", rand_search.best_params_)\n",
    "\n",
    "# --- 7. Final evaluation on test set ---\n",
    "# Use the best model from randomized search\n",
    "feat_test  # ensure feat_test is computed above\n",
    "\n",
    "y_pred = best_xgb.predict(feat_test)\n",
    "y_proba = best_xgb.predict_proba(feat_test)[:,1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --- 8. Plots ---. Plots ---\n",
    "\n",
    "# 8.1 Loss & Accuracy for CNN fine-tuning\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('CNN Fine-tuning Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('CNN Fine-tuning Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8.2 ROC Curve for XGBoost\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_disp = RocCurveDisplay(fpr=fpr, tpr=tpr)\n",
    "roc_disp.plot()\n",
    "plt.title('XGBoost ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# 8.3 Confusion Matrix for XGBoost\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "cm_disp.plot(cmap='Blues')\n",
    "plt.title('XGBoost Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb3uJ91ygemi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
